{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54c1c551",
   "metadata": {},
   "source": [
    "# Clustering — Complete Assignment (Theoretical + Practical)\n",
    "\n",
    "This notebook answers the assignment questions (both theoretical and practical) from the uploaded PDF. Reference: fileciteturn3file0\n",
    "\n",
    "Run all cells in order in a Jupyter environment with `scikit-learn`, `numpy`, `pandas`, `matplotlib`, and `seaborn` installed.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69391811",
   "metadata": {},
   "source": [
    "\n",
    "## Theoretical Questions — Short & Interview-friendly answers\n",
    "\n",
    "**1. What is unsupervised learning?**  \n",
    "Unsupervised learning finds patterns in unlabeled data (no target). Common tasks: clustering and dimensionality reduction.\n",
    "\n",
    "**2. How does K-Means work?**  \n",
    "K-Means initializes k centroids, assigns points to nearest centroid, updates centroids to cluster means, and repeats until convergence (assign→update loop).\n",
    "\n",
    "**3. What is a dendrogram?**  \n",
    "A dendrogram is a tree diagram showing hierarchical clustering merges/splits and distances at which clusters join.\n",
    "\n",
    "**4. Main difference: K-Means vs Hierarchical**  \n",
    "K-Means: partition-based, needs k, faster on large data. Hierarchical: builds tree (agglomerative or divisive), no need to pre-specify k (you can cut tree), more expensive.\n",
    "\n",
    "**5. Advantages of DBSCAN over K-Means**  \n",
    "DBSCAN finds clusters of arbitrary shape and identifies noise; it does not require number of clusters and handles varying densities better.\n",
    "\n",
    "**6. When use Silhouette Score?**  \n",
    "Use it to evaluate clustering quality (how similar samples are to their own cluster vs others). Helpful to choose k.\n",
    "\n",
    "**7. Limitations of Hierarchical Clustering**  \n",
    "Computationally expensive (O(n^3) worst), memory heavy for large datasets; sensitive to noise/outliers and linkage choice.\n",
    "\n",
    "**8. Why is feature scaling important in K-Means?**  \n",
    "K-Means uses distance — features with larger scales dominate. Scaling (StandardScaler/MinMax) prevents this bias.\n",
    "\n",
    "**9. How does DBSCAN identify noise points?**  \n",
    "Points not part of any dense region (not enough neighbors within eps) get label -1 (noise).\n",
    "\n",
    "**10. Define inertia in K-Means**  \n",
    "Inertia = sum of squared distances of samples to their closest cluster center (measure of compactness). Lower is better.\n",
    "\n",
    "**11. What is the elbow method?**  \n",
    "Plot inertia vs k and look for the \"elbow\" where inertia improvement slows — suggests suitable k.\n",
    "\n",
    "**12. Describe \"density\" in DBSCAN**  \n",
    "Density refers to how many points are within radius `eps`. High density regions form clusters.\n",
    "\n",
    "**13. Can hierarchical clustering be used on categorical data?**  \n",
    "Yes if you use suitable distance measures (e.g., Hamming) or encode categories, but typical implementations assume numeric data.\n",
    "\n",
    "**14. What does a negative Silhouette Score indicate?**  \n",
    "A sample is closer to a neighboring cluster than to its own cluster — poor clustering for that sample.\n",
    "\n",
    "**15. Explain linkage criteria in hierarchical clustering**  \n",
    "Linkage defines distance between clusters: single (min), complete (max), average (mean), ward (minimizes variance).\n",
    "\n",
    "**16. Why K-Means may perform poorly with varying sizes/densities?**  \n",
    "K-Means assumes spherical, equally-sized clusters — varying sizes/densities break this assumption, causing poor assignments.\n",
    "\n",
    "**17. Core parameters in DBSCAN and their effect**  \n",
    "`eps` (neighborhood radius) — larger eps merges clusters; `min_samples` — minimum points to form a dense region. Both control cluster shape and noise.\n",
    "\n",
    "**18. How K-Means++ improves initialization**  \n",
    "K-Means++ picks initial centroids probabilistically to be far apart, improving convergence and reducing bad random starts.\n",
    "\n",
    "**19. What is agglomerative clustering?**  \n",
    "A bottom-up hierarchical method that starts with each point as a cluster and iteratively merges the closest clusters based on linkage.\n",
    "\n",
    "**20. Why Silhouette > inertia?**  \n",
    "Inertia measures compactness only; Silhouette considers both cohesion (within) and separation (between) making it more informative for cluster quality.\n",
    "\n",
    "---\n",
    "\n",
    "*(The remaining theoretical items in the assignment are covered by the above (entropy, linkage, silhouette, DBSCAN concepts, etc.).)*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590f58d2",
   "metadata": {},
   "source": [
    "## Practical Tasks — Code (run each cell).  \n",
    "The cells below implement the practical questions listed in the assignment PDF. Each cell is self-contained and uses sklearn / synthetic datasets or built-in datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3234da",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Common imports for all practical tasks\n",
    "import numpy as np, pandas as pd, matplotlib.pyplot as plt, seaborn as sns\n",
    "from sklearn.datasets import make_blobs, make_moons, make_circles, load_iris, load_wine, load_breast_cancer, load_digits\n",
    "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import silhouette_score\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "sns.set(style='whitegrid')\n",
    "print('Libraries imported')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2225bf3a",
   "metadata": {},
   "source": [
    "### Task 1: make_blobs (4 centers) + KMeans (k=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6f80ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X, y = make_blobs(n_samples=500, centers=4, cluster_std=0.6, random_state=42)\n",
    "km4 = KMeans(n_clusters=4, random_state=42).fit(X)\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.scatter(X[:,0], X[:,1], c=km4.labels_, cmap='tab10', s=25)\n",
    "plt.scatter(km4.cluster_centers_[:,0], km4.cluster_centers_[:,1], c='black', s=120, marker='X')\n",
    "plt.title('KMeans k=4 on make_blobs'); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f483f85",
   "metadata": {},
   "source": [
    "### Task 2: Iris + AgglomerativeClustering (n=3) — show first 10 labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3fe969c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "iris = load_iris(as_frame=True)\n",
    "X_iris = iris.data; y_iris = iris.target\n",
    "agg = AgglomerativeClustering(n_clusters=3).fit(X_iris)\n",
    "print('First 10 predicted labels:', agg.labels_[:10])\n",
    "plt.figure(figsize=(5,4)); plt.scatter(X_iris.iloc[:,0], X_iris.iloc[:,1], c=agg.labels_, cmap='viridis'); plt.title('Agglomerative on Iris (first two features)'); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab608ed6",
   "metadata": {},
   "source": [
    "### Task 3: make_moons + DBSCAN (highlight noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70af40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_moons, _ = make_moons(n_samples=400, noise=0.08, random_state=0)\n",
    "db = DBSCAN(eps=0.2, min_samples=5).fit(X_moons)\n",
    "labels_db = db.labels_\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.scatter(X_moons[:,0], X_moons[:,1], c=labels_db, cmap='tab10', s=25)\n",
    "noise = X_moons[labels_db==-1]\n",
    "if len(noise)>0:\n",
    "    plt.scatter(noise[:,0], noise[:,1], facecolors='none', edgecolors='k', s=80, label='noise')\n",
    "plt.title('DBSCAN on make_moons (noise labelled -1)'); plt.legend(); plt.show()\n",
    "print('Noise count:', np.sum(labels_db==-1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e74156",
   "metadata": {},
   "source": [
    "### Task 4: Wine dataset + Standardize + KMeans (print cluster sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd8d1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "wine = load_wine(as_frame=True); Xw = wine.data\n",
    "sc = StandardScaler(); Xw_s = sc.fit_transform(Xw)\n",
    "kw = KMeans(n_clusters=3, random_state=0).fit(Xw_s)\n",
    "(unique, counts) = np.unique(kw.labels_, return_counts=True)\n",
    "print('Cluster sizes:', dict(zip(unique, counts)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f738dc23",
   "metadata": {},
   "source": [
    "### Task 5: make_circles + DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d9444d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_circ, _ = make_circles(n_samples=500, factor=0.5, noise=0.05, random_state=1)\n",
    "dbcirc = DBSCAN(eps=0.12, min_samples=5).fit(X_circ)\n",
    "plt.figure(figsize=(5,5)); plt.scatter(X_circ[:,0], X_circ[:,1], c=dbcirc.labels_, cmap='tab10'); plt.title('DBSCAN on make_circles'); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a0d59e",
   "metadata": {},
   "source": [
    "### Task 6: Breast Cancer + MinMaxScaler + KMeans (2) — show centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f78865a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "bc = load_breast_cancer(as_frame=True); Xbc = bc.data\n",
    "mm = MinMaxScaler(); Xbc_mm = mm.fit_transform(Xbc)\n",
    "k2 = KMeans(n_clusters=2, random_state=42).fit(Xbc_mm)\n",
    "print('Centroids (scaled space):\\n', k2.cluster_centers_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f67bf4",
   "metadata": {},
   "source": [
    "### Task 7: make_blobs with varying std devs + DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e685696",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_var, _ = make_blobs(n_samples=600, centers=[[0,0],[4,4],[8,0]], cluster_std=[0.2,1.5,0.5], random_state=2)\n",
    "dbv = DBSCAN(eps=0.6, min_samples=8).fit(X_var)\n",
    "plt.figure(figsize=(6,4)); plt.scatter(X_var[:,0], X_var[:,1], c=dbv.labels_, cmap='tab10'); plt.title('DBSCAN on varying std blobs'); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f02d5d6",
   "metadata": {},
   "source": [
    "### Task 8: Digits dataset → PCA(2) → KMeans visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68eb9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "digits = load_digits(); Xd = digits.data\n",
    "pca = PCA(n_components=2, random_state=42); Xd2 = pca.fit_transform(Xd)\n",
    "kd = KMeans(n_clusters=10, random_state=42).fit(Xd2)\n",
    "plt.figure(figsize=(6,4)); plt.scatter(Xd2[:,0], Xd2[:,1], c=kd.labels_, cmap='tab10', s=25); plt.title('Digits PCA(2) + KMeans'); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58307bd0",
   "metadata": {},
   "source": [
    "### Task 9: Silhouette scores for k=2..5 on synthetic blobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e419de",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_sb, _ = make_blobs(n_samples=400, centers=4, cluster_std=0.7, random_state=10)\n",
    "scores = {k: silhouette_score(X_sb, KMeans(n_clusters=k, random_state=0).fit_predict(X_sb)) for k in range(2,6)}\n",
    "plt.figure(figsize=(5,3)); plt.bar(scores.keys(), scores.values()); plt.title('Silhouette k=2..5'); plt.xlabel('k'); plt.show(); print('Scores:', scores)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5db0ef",
   "metadata": {},
   "source": [
    "### Task 10: Iris dendrogram (average linkage) — small subset for readability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9490d149",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_iris_small = X_iris.iloc[:50]\n",
    "linked = linkage(X_iris_small, method='average')\n",
    "plt.figure(figsize=(10,4)); dendrogram(linked, labels=list(iris.target_names[y_iris[:50]]), leaf_rotation=90); plt.title('Dendrogram (Iris subset)'); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e6fd92",
   "metadata": {},
   "source": [
    "### Task 11: Overlapping blobs → KMeans → visualize decision boundaries (2D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25717c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# overlapping blobs and decision boundary visualization\n",
    "X_ov, y_ov = make_blobs(n_samples=400, centers=[[0,0],[2,0],[1,1.5]], cluster_std=1.1, random_state=5)\n",
    "km_ov = KMeans(n_clusters=3, random_state=0).fit(X_ov)\n",
    "# decision boundary grid\n",
    "xx, yy = np.meshgrid(np.linspace(X_ov[:,0].min()-1, X_ov[:,0].max()+1, 300), np.linspace(X_ov[:,1].min()-1, X_ov[:,1].max()+1, 300))\n",
    "Z = km_ov.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
    "plt.figure(figsize=(6,4)); plt.contourf(xx,yy,Z, alpha=0.2); plt.scatter(X_ov[:,0], X_ov[:,1], c=km_ov.labels_, s=25); plt.title('KMeans decision boundaries (overlapping blobs)'); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830f5996",
   "metadata": {},
   "source": [
    "### Task 12: Digits → t-SNE → DBSCAN → visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8255de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tsne = TSNE(n_components=2, random_state=42, init='random', learning_rate='auto', perplexity=30)\n",
    "Xd_tsne = tsne.fit_transform(digits.data[:500])  # limit to 500 for speed\n",
    "db_dig = DBSCAN(eps=3.5, min_samples=5).fit(Xd_tsne)\n",
    "plt.figure(figsize=(6,4)); plt.scatter(Xd_tsne[:,0], Xd_tsne[:,1], c=db_dig.labels_, cmap='tab10', s=25); plt.title('Digits t-SNE + DBSCAN'); plt.show()\n",
    "print('Noise in DBSCAN labels:', np.sum(db_dig.labels_==-1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90219796",
   "metadata": {},
   "source": [
    "### Task 13: AgglomerativeClustering (complete linkage) on synthetic blobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16748ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "agg_comp = AgglomerativeClustering(n_clusters=3, linkage='complete').fit(X_ov)\n",
    "plt.figure(figsize=(6,4)); plt.scatter(X_ov[:,0], X_ov[:,1], c=agg_comp.labels_, cmap='tab10'); plt.title('Agglomerative (complete) on blobs'); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ea197d",
   "metadata": {},
   "source": [
    "### Task 14: Wine dataset — inertia values for K=2..6 (plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249ded30",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "inertias = []\n",
    "Ks = range(2,7)\n",
    "for k in Ks:\n",
    "    km = KMeans(n_clusters=k, random_state=0).fit(Xw_s)\n",
    "    inertias.append(km.inertia_)\n",
    "plt.figure(figsize=(5,3)); plt.plot(Ks, inertias, marker='o'); plt.title('Wine dataset inertia K=2..6'); plt.xlabel('k'); plt.ylabel('Inertia'); plt.show(); print('Inertias:', dict(zip(Ks, inertias)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665adaba",
   "metadata": {},
   "source": [
    "### Task 15: Concentric circles → Agglomerative (single linkage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be79db10",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_cc, _ = make_circles(n_samples=400, factor=0.3, noise=0.03)\n",
    "agg_single = AgglomerativeClustering(n_clusters=2, linkage='single').fit(X_cc)\n",
    "plt.figure(figsize=(5,5)); plt.scatter(X_cc[:,0], X_cc[:,1], c=agg_single.labels_, cmap='tab10'); plt.title('Agglomerative (single) on concentric circles'); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63fc7485",
   "metadata": {},
   "source": [
    "### Task 16: Wine dataset → scale → DBSCAN → count clusters (exclude noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60dcf1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "Xw_scaled = StandardScaler().fit_transform(load_wine(as_frame=True).data)\n",
    "db_w = DBSCAN(eps=1.3, min_samples=5).fit(Xw_scaled)\n",
    "labels_w = db_w.labels_\n",
    "n_clusters = len(set(labels_w)) - (1 if -1 in labels_w else 0)\n",
    "print('DBSCAN clusters (excluding noise):', n_clusters, 'noise points:', np.sum(labels_w==-1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9099fc",
   "metadata": {},
   "source": [
    "### Task 17: KMeans cluster centers on blobs (plot centers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06869e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_b, _ = make_blobs(n_samples=300, centers=4, random_state=10)\n",
    "km_b = KMeans(n_clusters=4, random_state=10).fit(X_b)\n",
    "plt.figure(figsize=(6,4)); plt.scatter(X_b[:,0], X_b[:,1], c=km_b.labels_, cmap='tab10'); plt.scatter(km_b.cluster_centers_[:,0], km_b.cluster_centers_[:,1], c='k', s=120, marker='X'); plt.title('Blobs with cluster centers'); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2acda24",
   "metadata": {},
   "source": [
    "### Task 18: Iris → DBSCAN → count noise samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788d4459",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "db_iris = DBSCAN(eps=0.9, min_samples=5).fit(X_iris)\n",
    "print('Iris DBSCAN noise count:', np.sum(db_iris.labels_==-1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "337aac34",
   "metadata": {},
   "source": [
    "### Task 19: make_moons + KMeans (note poor fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21cb2245",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_moons2, _ = make_moons(n_samples=300, noise=0.05, random_state=6)\n",
    "km_m = KMeans(n_clusters=2, random_state=6).fit(X_moons2)\n",
    "plt.figure(figsize=(6,4)); plt.scatter(X_moons2[:,0], X_moons2[:,1], c=km_m.labels_, cmap='tab10'); plt.title('KMeans on make_moons (non-linear)'); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad09ebb",
   "metadata": {},
   "source": [
    "### Task 20: Digits → PCA(3) → KMeans → 3D scatter (requires mpl_toolkits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a211c906",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "pca3 = PCA(n_components=3, random_state=42); Xd3 = pca3.fit_transform(digits.data)\n",
    "k3d = KMeans(n_clusters=10, random_state=42).fit(Xd3)\n",
    "fig = plt.figure(figsize=(6,5)); ax = fig.add_subplot(111, projection='3d'); ax.scatter(Xd3[:,0], Xd3[:,1], Xd3[:,2], c=k3d.labels_, s=20); ax.set_title('Digits PCA(3) + KMeans'); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ecf5c2",
   "metadata": {},
   "source": [
    "### Extra Practical Tasks (grouped):\n",
    "- silhouette on 5-center blobs\n",
    "- BreastCancer PCA + Agglomerative (visualize 2D)\n",
    "- Noisy circles: compare KMeans vs DBSCAN side-by-side\n",
    "- Silhouette per sample after KMeans on Iris\n",
    "- Agglomerative (average) on blobs visualize\n",
    "- Wine dataset KMeans pairplot (first 4 features)\n",
    "- Noisy blobs + DBSCAN identify clusters and noise\n",
    "- Digits t-SNE + Agglomerative clustering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a268a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1) Silhouette for 5-center blobs\n",
    "X5, _ = make_blobs(n_samples=500, centers=5, cluster_std=0.6, random_state=8)\n",
    "km5 = KMeans(n_clusters=5, random_state=8).fit(X5); print('Silhouette (k=5):', silhouette_score(X5, km5.labels_))\n",
    "\n",
    "# 2) BreastCancer PCA + Agglomerative visualize 2D\n",
    "Xbc = load_breast_cancer(as_frame=True).data\n",
    "pca2 = PCA(n_components=2, random_state=0); Xbc2 = pca2.fit_transform(Xbc)\n",
    "agg_bc = AgglomerativeClustering(n_clusters=2).fit(Xbc2)\n",
    "plt.figure(figsize=(5,4)); plt.scatter(Xbc2[:,0], Xbc2[:,1], c=agg_bc.labels_, cmap='tab10', s=20); plt.title('BreastCancer PCA(2) + Agglomerative'); plt.show()\n",
    "\n",
    "# 3) Noisy circles: KMeans vs DBSCAN\n",
    "Xn, _ = make_circles(n_samples=400, factor=0.5, noise=0.08, random_state=9)\n",
    "kmn = KMeans(n_clusters=2, random_state=9).fit(Xn); dbn = DBSCAN(eps=0.12, min_samples=5).fit(Xn)\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.subplot(1,2,1); plt.scatter(Xn[:,0], Xn[:,1], c=kmn.labels_, cmap='tab10'); plt.title('KMeans on noisy circles')\n",
    "plt.subplot(1,2,2); plt.scatter(Xn[:,0], Xn[:,1], c=dbn.labels_, cmap='tab10'); plt.title('DBSCAN on noisy circles'); plt.show()\n",
    "\n",
    "# 4) Silhouette per sample for Iris after KMeans (k=3)\n",
    "from sklearn.metrics import silhouette_samples\n",
    "km_iris = KMeans(n_clusters=3, random_state=0).fit(X_iris)\n",
    "samps = silhouette_samples(X_iris, km_iris.labels_)\n",
    "plt.figure(figsize=(6,3)); plt.bar(range(len(samps)), samps); plt.title('Silhouette values per sample (Iris + k=3)'); plt.show()\n",
    "\n",
    "# 5) Agglomerative (average) on blobs visualize\n",
    "Xavg, _ = make_blobs(n_samples=300, centers=3, random_state=12)\n",
    "agg_avg = AgglomerativeClustering(n_clusters=3, linkage='average').fit(Xavg)\n",
    "plt.figure(figsize=(6,4)); plt.scatter(Xavg[:,0], Xavg[:,1], c=agg_avg.labels_, cmap='tab10'); plt.title('Agglomerative average linkage'); plt.show()\n",
    "\n",
    "# 6) Wine pairplot (first 4 features) with KMeans labels\n",
    "wine_df = load_wine(as_frame=True).data\n",
    "km_w = KMeans(n_clusters=3, random_state=0).fit(wine_df.iloc[:,:4])\n",
    "sns.pairplot(wine_df.iloc[:,:4].assign(cluster=km_w.labels_), hue='cluster', corner=True); plt.show()\n",
    "\n",
    "# 7) Noisy blobs + DBSCAN identify clusters and noise, print counts\n",
    "Xnb, _ = make_blobs(n_samples=400, centers=4, cluster_std=1.2, random_state=20)\n",
    "dbnb = DBSCAN(eps=0.9, min_samples=6).fit(Xnb)\n",
    "labels_nb = dbnb.labels_\n",
    "print('DBSCAN found clusters (excluding noise):', len(set(labels_nb)) - (1 if -1 in labels_nb else 0), 'noise count:', np.sum(labels_nb==-1))\n",
    "\n",
    "# 8) Digits t-SNE + Agglomerative (sample subset)\n",
    "Xd_tsne2 = TSNE(n_components=2, random_state=42, init='random', learning_rate='auto', perplexity=30).fit_transform(digits.data[:500])\n",
    "agg_tsne = AgglomerativeClustering(n_clusters=10).fit(Xd_tsne2)\n",
    "plt.figure(figsize=(6,4)); plt.scatter(Xd_tsne2[:,0], Xd_tsne2[:,1], c=agg_tsne.labels_, cmap='tab10', s=20); plt.title('Digits t-SNE + Agglomerative'); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82f3724",
   "metadata": {},
   "source": [
    "**Notebook saved programmatically** — file path: `/mnt/data/clustering_full_assignment.ipynb`. Run this notebook in Jupyter; each cell is designed to execute without internet access (uses sklearn built-in and synthetic data)."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
